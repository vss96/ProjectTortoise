{"distinguished":null,"retrieved_on":1473809390,"gilded":0,"id":"cw17m","edited":false,"parent_id":"t1_cw06o","author":"dbenhur","author_flair_text":null,"score":1,"ups":1,"author_flair_css_class":null,"created_utc":1167112088,"subreddit":"programming","subreddit_id":"t5_2fwo","link_id":"t3_vzbb","stickied":false,"body":"&gt; explaining why he's lost 200K records because the update to disk was lagging by two hours when Billy Bob tripped over the power cable.\r\n\r\nWhy would a transaction volume of 1M recs/day be lagged by 2 hours on his system?  That SATA disk can probably write about 10-20K rows/sec as raw data, probably about 100-200/sec as single row inserts with a pair of indexes to update.  1M rows over 8 hours averages 35 inserts/second.\r\n\r\nBesides, in TFA he already stated that his data was originating from elsewhere and being batched in.  In fact half the rant was his explaining about his bulk insert process.\r\n\r\n&gt; why performance goes from excellent to abysmal when the db exceeds available memory.\r\n\r\nHe doesn't need to keep the whole dataset in memory, indexing actually works, you know.  The big memory solution will degrade gracefully when the database gets bigger.\r\n\r\nAgain, despite the author's flamboyant description, I contend the article's problem is neither large or hard with modern hardware and software.\r\n","controversiality":0,"timestamp_epoch":1566236778,"message_id":"50720-ff6566d1-b305-4197-b83f-ae665dd26db2-1566236778"}