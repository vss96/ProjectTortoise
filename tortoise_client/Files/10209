{"subreddit":"reddit.com","author_flair_css_class":null,"created_utc":1165871280,"score":1,"ups":1,"controversiality":0,"body":"\r\nThe whole Searle-AI community debate can perhaps be compressed to these points. ( Add more if you like to )\r\n\r\nQ 1) Is learning contextually related to the entity that learns? Does it have cognitive structures that recognise and make sense of what it is doing? Does it understand. To paraphrase Chalmer this is the easy problem.\r\n\r\nQ 2) Is thinking equivalent to learning? If so, what makes you think so? If not what is it that differentiates them? If thinking is different from learning, what is thought? That of course leads to the question as to who is it that thinks. This to paraphrase Chalmers again is the hard problem.\r\n\r\nA 1) The first question is easier to answer, when AI's entities have cognitive stores and when its cognition improves, the data and processing can become relevant to the entity. A simple feedback loop in such an entity can help it become practically autonomous. \r\n\r\nThere are already machines that exhibit limited amounts of such cognition and as our knowledge of cognition grows these systems can be improved. However cognition can be more processor intensive compared to plain math based inference.(there is a reason for this too) \r\n\r\nUnderstanding means cognition + inference, in AI cognition is severely limited in comparision to inference. This is more because with our knowledge of math, our knowledge of inference is better than our cognition. I know that is a simplisitic answer but it conveys the essence. However as we see in the next point that such understanding is not really enough to answer our second question.\r\n\r\nA.2) Many of us tend to assume that thinking and learning are the same. Surely all of life learns, from viruses to man and surely you cannot say that the virus is a thinking being. If so bring on the (viral) math?:):)Our best AI machines are only learning machines not thinking machines. \r\n\r\nObviously there is a difference between thinking and learning and that difference comes from the presence of mind. Rather than argue about the intricacies of qualia, consciousness etc, perhaps AI people can wonder about a learning architecture than can give rise to such a condition where thinking knows about its learning. \r\n\r\nLet us put it this way, all of us from viruses to humans are learning machines, however those with thought see these learning processes. So if we call the learning processes implicit and an exposed view of them as explicit, we say that thinking machines learn in two ways, one by implicit learning and two by explicit learning. \r\n\r\nSymbols mark out contexts and learning processes, language allows the entity to describe what it sees, our math and science separate the wheat from the chaff. we use language to do futher learning. Why is it that only humans as a species use symbols to denote contexts and solution paths? Notice that we know little about how we recognise, also notice that the base of any cognition differs from individual to individual.\r\n\r\nAn example that explains how the thinking process sees the learning process may be with respect to an IDE. \r\n\r\nIn a program running in the IDE, the program reacts to the state of its variables, it does not see its solution paths, only its choices. The programmer on the other hand sees both, the program reacting to variable states and the program path which he has made. \r\n\r\nThis knowledge of the program path allows him to alter them to his ends, which however the program cannot do. Now imagine a situation where the programmer is also the program, ( vice versa is better) then the program(mer) can see both the variable states and the program paths and can seek to alter them; that is if his ends dictate the means(very important)\r\n\r\nThis is only part of the answer. As you can see these things are intimately connected, the problem of consciousness, mind, language, thought, the rise of science and math and intelligent machines. Searle's point about consciousness and intent is very much valid.\r\n\r\nAI in its infancy was akin to a small enthusiastic child that assumed things that do not really hold and Searle like a school teacher had to point it out. \r\n\r\nUnderstanding that however doesnt solve the problem he pointed out. Therefore I feel that we do not really need more refutations of Searle's points. His points are valid in the sense earlier referred to, what we need are fresh thoughts on them. That refutations are still coming by almost 2 decades after the problem point to the fact that there has been little fresh thought on the subject. is this a repeat of zeno and physics/math?\r\n\r\nHope that this allows all of us here to take a different look at the entire problem. the solution as you can see is partly here. \r\n\r\nI read an article on New Order, the web site (security web site?) where an AI (possibly amateur) author bemoans the fact that many people whom he talked to did not agree that machines could think and proceeds to say and prove that they could. \r\n\r\nI wasn't surprised by his line of thinking, only he like many others does not recognise that thinking is a different ball game... Our AI machines may arguable be the product of our thoughts but to say that they think.... well we are not yet there.... let us think about getting there first. \r\n\r\nif you would like to discuss this or write to me, write back to gayathreeu at gmail dot com. \r\n\r\n---------\r\n* Language, Symbols, Animals/other life - OK some chimps seem to be able to do symbolic language /manipulation, we should be really seeing more animals doing that, IMHO....\r\n\r\n** Symbols, Speaking Programs, Understanding - And the point that Searle objected to that syntax does not suffice does hold. Speaking programs (ELIZA?) and the parrot are no different, it could even be argued that the parrot has a better sense of cognition and understanding than ELIZA. Like when Greg and Lissie are together and the parrot repeatedly calls out \"I love you Fred\". Greg takes a look at Lissie who immdl'y blanches, well ....\r\n\r\nAI entities of course do use symbols but not in the way or sense we humans use them. In humans, the symbol is not the structure, as Searle points out with his syntax vs semantics, the symbol is the distillation of the structure and like the apple and apple juice, these are not equivalent though they spring from the same source.  \r\n\r\n*** Than this refutation to Searle, McCarthy has more valid points in his more thoughtful paper on robot consciousness, (I remember reading another of his papers where he does partly agree with some of Searle's points, I do not remember if it is this paper, too damn lazy to reread it and it was some time back)\r\n","link_id":"t3_jnf7","stickied":false,"subreddit_id":"t5_6","gilded":0,"retrieved_on":1473808331,"distinguished":null,"author":"thethinker","author_flair_text":null,"parent_id":"t3_jnf7","id":"cuhf9","edited":false,"timestamp_epoch":1566236766,"message_id":"23202-d6e0c93c-2a7b-47ee-895c-72d198b4a3b6-1566236766"}