{"body":"A utility function that is purely linear in money? I'd call that a WTF utility function, not bearing much resemblance to most human mindsets, selfish or altruistic.\r\n\r\nTo clarify what I'm talking about when I'm speaking of a selfish or altruistic utility function, I mean the \"top level goals\" utility function.\r\n\r\nTo clarify the distinction... When you have to go somewhere, you may have a restricted utility function that places high utility on stuff like \"find car keys\"... But that wouldn't be your ultimate utility function, would it? It would be more a representaton of subgoals.\r\n\r\nie, a highest level utility function would be \"maximize personal pleasure\" or \"greatest good for greatest number of people\" or something entirely different.\r\n\r\nNow, you seem to be making the argument that all utility functions are selfish in the sense that one ultimately acts for the \"reward\" of doing what they want, be it something normally considered altruistic or selfish.\r\n\r\nBut our build in reinforcement system is just an _implementation_, not the goal itself. By arguing that, you're redefining your ultimate utility function in midargument. You say to assume U = km + cu... then later you you assert that U = emotional pleasure/satisfaction or whatever.\r\n\r\nIf you chose the first U, then what you're calling the second U would be an implementation detail. not the actual utility function. If the second U is what you're calling your actual U, then the first U is really not U, but just a means toward maximizing U.\r\n\r\nThere're a few ways to make the distinction clearer. One way I've seen suggested is this:\r\nAssume a purely rational actor with some supposedly altruistic (in some sense) top level utility function U. Further, assume that this actor believes that no afterlife of any sort exists. ie, assume that this actor assumes that their existance completely ceases upon death. No consiousness or anything of any sort.\r\n\r\nThen the distinction between a utility function that is actually \"maximize personal emotional pleasure/satisfaction\" vs something based on some higher ideal (say, something that involves acting for the sake of others) would be this: Would the actor potentially willingly sacrafice themselves ever, say, for the sake of another?\r\n\r\nRemember, we are hypothesizing a purely rational actor who also believes that upon death they cease to exist. This actor would then expect that the action of sacraficing themselves would mean never feeling any further \"happy fuzzies\"... because they'd no longer exist.\r\n\r\nSo if maximizing their personal \"happy fuzzies\" was the goal, they would not sacrafice themselves. In other words, a rational actor with such beliefs that would even so willingly sacrafice themselves for the sake of another, or for some higher cause, logically cannot possibly have a utility function of the form of abstract selfishness you have described. ie, it cannot be \"for the kicks\" as the highest utility.\r\n\r\nAnother way of differentiating is this... assuming the tech exists... would some rational actor (not necessarally the same one as in the above example) \"wirehead\"? ie, implant stuff into your brain that woud end up just continually 'prod' your pleasure centers? Assume the system would also be set up to be able to support the actor's life indefinately.\r\n\r\nIf the actor's top level goal, ie, ultimate utility function was along the lines of \"maximize personal pleasure\", then the actor absolutely would do this. If a rational actor refused this, then it would be reasonable to assume that their utility function was something else _other_ than simply maximizing personal pleasure, such that sitting around permanently in a vegatative blissed out state would have a low expected utility.","controversiality":0,"subreddit_id":"t5_6","link_id":"t3_w3n5","stickied":false,"subreddit":"reddit.com","score":7,"ups":7,"author_flair_css_class":null,"created_utc":1167250662,"author":"Psy-Kosh","author_flair_text":null,"id":"cw6lf","edited":false,"parent_id":"t1_cw4vs","gilded":0,"distinguished":null,"retrieved_on":1473809493,"timestamp_epoch":1566236778,"message_id":"53622-f0a19c29-0730-4a0a-9d89-2ff1b9bc1bf1-1566236778"}