{"author_flair_css_class":null,"created_utc":1167257638,"score":2,"ups":2,"subreddit":"programming","link_id":"t3_vzbb","stickied":false,"subreddit_id":"t5_2fwo","body":"&gt; The expected spatial behavior of a b-tree index is O(n log n), iirc\r\n\r\nWell, you're talking a binary tree, but I'm fine with using that for theoretical purposes.\r\n\r\nFriends don't let friends play with Russian roulette with math formulas. The actual formula is 2n-1. I think you were quoting the execution time for a heap sort.\r\n\r\nYou can derive it as follows: number of leaves at the bottom most level is n, so the number of levels is log2(n). The total number of nodes at any given level is 2^L (where L is the level number). So you end up with a geometric series starting at looking like 2^0 + 2^1 + 2^2 + ... + 2^L (I should really remember how to do summation notation in HTML ;-). Anyway, that reduces all down to 2^(L+1)-1 nodes in a tree. Substitute in log2(n) for L and reduce, and you end up with 2n-1. Which makes a lot more sense because it's hard to imagine how adding one more leaf to a binary tree could cause you to need to add more than one other node (which is where nlog(n) really falls apart).\r\n\r\nIf you doubt the math, try building a binary tree with 8 leaf nodes that needs 24 nodes instead of 15. ;-)\r\n\r\n&gt; (actually, it varies based on the number of possible brother nodes, but in this example that will be constant).\r\n\r\nIt's important to keep that in mind though, because you have to consider what the difference really is between this partitioning and having a b-tree whose root node has 26 children. (Answer: not much besides better balancing and therefore space efficiency. ;-)\r\n\r\nB-tree's are effectively a collusion between bucketing/hashing and tree structures. When one imposes one's own bucketing or tree in to them, one creates inefficiencies for the general case which had better pay off for the special case one is optimizing for.\r\n\r\n&gt; 2.66 x 10^9 vs. 2.19 x 10^9\r\n\r\nSo, simple thought experiment: let's build a binary tree with 32 leaf nodes, and we'll stick with your nlog(n) formula for amusement's sake. That means you need 160 nodes (actual number is 63, but that just makes this all the more sad). Now, off of 26 of those leaf nodes, we hang one of your bucketed trees. Now I've got a tree with all the nodes, and I needed only 160 nodes more to do the job, which is more than 400 million fewer than you predicted. Of course, the tree isn't properly balanced, but balancing wouldn't make it worse.\r\n\r\nSo, when you use the actual formula, it turns out like this:\r\n\r\n2(10^8) - 1 vs. 26*(2(3.85x10^6) - 1)\r\n\r\nor about\r\n\r\n2 x 10^8 vs. 2 x 10^8\r\n\r\nwhich makes sense after you consider my thought experiment above.\r\n\r\nOf course, in reality, letter usage varies significantly in most languages. \"e\" is about 50 times as common as \"q\" or \"j\". I don't know the stats for \"words that start with\", but a quick check in /usr/share/dict/words suggests \"s\" has about 80x more words than \"x\". Maybe for stock symbols it's a bit more even, but you have to think that the number of rows in these partitions are going to vary significantly enough that you will have a observable (although perhaps trivial) increase in the index size. Even if you do have perfectly even distributions, the best you are going to achieve is beating the binary tree by 52 nodes, which is statistical noise.\r\n\r\n&gt; I agree. Mostly, i think it depends on the RAM overhead. If he can fit the relevant indices into RAM for each query, there shouldn't be much behavioral difference.\r\n\r\nWell, people tend to measure DB performance in terms of disk access, so in that context you are right. However, even if they all fit in memory, there are the CPU costs of doing the compares. Now, bucketing is going to give you fewer compares than a binary tree, but a B-tree with the appropriate sizing parameters should be able to save you one or two node lookups against the worst (i.e. biggest) bucket, assuming you count the partitioning as your first node lookup (which from a CPU consumption perspective it very much is). On average, the balanced tree should save you some relatively small number of node lookups (quite possibly less than one) per query, which may sound trivial until you consider that with 10^8 records you're only doing 5-6 lookups with a 26-sibilings b-tree, so that can be a 10-20% improvement in lookup speed. Dwarfed by disk I/O once you actually touch the record, yes, but that assumes that you can't amortize that cost (easier to do with reads than writes, and easier to do if your indexes aren't eating up all your RAM... the wins just keep on coming!).\r\n\r\nSo in summary: partitioning as a way to speed up single record access times on a single drive is an exercise in shooting yourself in the foot.","controversiality":0,"retrieved_on":1473809500,"distinguished":null,"gilded":0,"parent_id":"t1_cw5rf","edited":false,"id":"cw6z6","author":"xcbsmith","author_flair_text":null,"timestamp_epoch":1566236779,"message_id":"53834-34c767ef-9bf9-4f5e-af24-7df595ae1319-1566236779"}