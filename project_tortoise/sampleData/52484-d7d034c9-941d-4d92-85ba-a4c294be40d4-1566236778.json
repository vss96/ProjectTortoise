{"gilded":0,"distinguished":null,"retrieved_on":1473809453,"author_flair_text":null,"author":"Psy-Kosh","id":"cw4iq","edited":false,"parent_id":"t1_cw49n","subreddit":"reddit.com","score":14,"ups":14,"created_utc":1167202162,"author_flair_css_class":null,"controversiality":0,"body":"That's unlikely to maximize utility unless your utility function is simply representative of maximal selfishness.\r\n\r\nAnd even then, it becomes shaky. (game theory, etc).\r\n\r\nAlso, there is nothing obviously inherently more rational about being selfish than being altruistic.\r\n\r\nEDIT: slightly modified third statement to clarify it.","subreddit_id":"t5_6","stickied":false,"link_id":"t3_w3n5","timestamp_epoch":1566236778,"message_id":"52484-d7d034c9-941d-4d92-85ba-a4c294be40d4-1566236778"}